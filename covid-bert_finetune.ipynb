{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "   AutoConfig,\n",
    "   AutoModel,\n",
    "   AutoTokenizer,\n",
    "   TFAutoModelForSequenceClassification,\n",
    "   AdamW\n",
    "#    glue_convert_examples_to_features\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import json\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('labeled/labeled.tsv', sep = '\\t')\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'].to_list(), df['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model\n",
    "# @markdown >The default model is <i><b>COVID-Twitter-BERT</b></i>. You can however choose <i><b>BERT Base</i></b> or <i><b>BERT Large</i></b> to compare these models to the <i><b>COVID-Twitter-BERT</i></b>. All these three models will be initiated with a random classification layer. If you go directly to the Predict-cell after having compiled the model, you will see that it still runs the predition. However the output will be random. The training steps below will finetune this for the specific task. <br /><br /> \n",
    "model_name = 'digitalepidemiologylab/covid-twitter-bert' #@param [\"digitalepidemiologylab/covid-twitter-bert\", \"bert-large-uncased\", \"bert-base-uncased\"]\n",
    "\n",
    "# Initialise tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_fn(text_list):\n",
    "    t_tokenizer = tokenizer(\n",
    "        text_list,\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = max_seq_length,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    input_ids = t_tokenizer['input_ids']\n",
    "    token_type_ids = t_tokenizer['token_type_ids']\n",
    "    attention_mask = t_tokenizer['attention_mask']\n",
    "    return input_ids,token_type_ids,attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramteters\n",
    "#@markdown >Batch size and sequence length needs to be set to prepare the data. The size of the batches depends on available memory. For Colab GPU limit batch size to 8 and sequence length to 96. By reducing the length of the input (max_seq_length) you can also increase the batch size. For a dataset like SST-2 with lots of short sentences. this will likely benefit training.\n",
    "max_seq_length = 96 #@param {type: \"integer\"}\n",
    "train_batch_size = 8 #@param {type: \"integer\"} \n",
    "eval_batch_size = 8 #@param {type: \"integer\"}\n",
    "\n",
    "\n",
    "#@markdown >The Glue dataset has around 62000 examples, and we really do not need them all for training a decent model. To cut down training time, please reduse this to only a percentage of the entire set.\n",
    "use_percentage_of_data = 100 #@param {type: \"slider\", min: 1, max: 100}\n",
    "\n",
    "# get dataset sizes\n",
    "num_train_examples = len(X_train)\n",
    "num_dev_examples = len(X_test)\n",
    "num_labels = 2\n",
    "\n",
    "# Map the labels for printing\n",
    "label_mapping = {\"0\": 0, \"1\": 1}\n",
    "\n",
    "print(f'\\n\\nThe dataset is downloaded. The entire dataset has {num_train_examples + num_dev_examples} examples of which you are using {use_percentage_of_data}%. This will result in a train dataset with {int(num_train_examples * (use_percentage_of_data/100))} examples and a validation dataset with {int(num_dev_examples * (use_percentage_of_data/100))} examples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids, train_token_type_ids, train_attention_mask = encode_fn(X_train)\n",
    "dev_input_ids, dev_token_type_ids, dev_attention_mask = encode_fn(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown >The default learning rate of 2e5 will be fine in most cases\n",
    "learning_rate = 2e-5 #@param {type: \"number\"}\n",
    "\n",
    "#@markdown > Typically these type of models are finetuned for 3 epochs. This can be increased for small datasets and decreased for large datasets.\n",
    "num_epochs = 1  #@param {type: \"integer\"}\n",
    "\n",
    "# Initialise a Model for Sequence Classification with 2 labels\n",
    "config = AutoConfig.from_pretrained(model_name, num_labels=num_labels)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Metrics and callbacks\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
    "checkpoint_path = './checkpoints/checkpoint.{epoch:02d}'\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True)]\n",
    "\n",
    "# Compute some variables\n",
    "train_steps_per_epoch = int(num_train_examples * (use_percentage_of_data/100) / train_batch_size)\n",
    "dev_steps_per_epoch = int(num_dev_examples * (use_percentage_of_data/100) / eval_batch_size)\n",
    "\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "  [train_input_ids, train_token_type_ids, train_attention_mask], tf.constant(y_train.to_list()),\n",
    "  epochs=num_epochs,\n",
    "  validation_data=([dev_input_ids, dev_token_type_ids, dev_attention_mask], tf.constant(y_test.to_list())),\n",
    "  callbacks=callbacks)\n",
    "\n",
    "# Print some information about the training\n",
    "print(f'\\nThe training has finished training after {num_epochs} epochs.')\n",
    "print('\\nThe history contains the accuracy and loss at every epoch:')\n",
    "print(json.dumps(history.history, indent=4))\n",
    "\n",
    "print('\\nThe checkpoint callback has generated a checkpoint after every epoch (loss being the training loss, val_loss is the validation loss):')\n",
    "!ls -lha ./checkpoints/\n",
    "\n",
    "print('\\nWe will now save the finetuned model and the corresponding config file on your Colab disk.')\n",
    "model.save_pretrained('./huggingface_model/')\n",
    "\n",
    "print('\\nTensorflow model and config-file is saved in ./huggingface_model/')\n",
    "!ls -lha ./huggingface_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small function only used for formatting the output\n",
    "def format_prediction(preds, label_mapping, label_name):\n",
    "    preds = tf.nn.softmax(preds[0], axis=1)\n",
    "    formatted_preds = []\n",
    "    for pred in preds.numpy():\n",
    "        # convert to Python types and sort\n",
    "        pred = {label: float(probability) for label, probability in zip(label_mapping.values(), pred)}\n",
    "        pred = {k: v for k, v in sorted(pred.items(), key=lambda item: item[1], reverse=True)}\n",
    "        formatted_preds.append({label_name: list(pred.keys())[0], f'{label_name}_probabilities': pred})\n",
    "    return formatted_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = model(train_input_ids)\n",
    "formatted_preds = format_prediction(train_pred, label_mapping, 'antivax')\n",
    "train_p = [p['antivax'] for p in formatted_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(y_true=y_train, y_pred=train_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(y_true=y_train, y_pred=train_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.precision_recall_fscore_support(y_true=y_train, y_pred=train_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model(dev_input_ids)\n",
    "formatted_preds = format_prediction(test_pred, label_mapping, 'antivax')\n",
    "test_p = [p['antivax'] for p in formatted_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(y_true=y_test, y_pred=test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(y_true=y_test, y_pred=test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.precision_recall_fscore_support(y_true=y_test, y_pred=test_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conservatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_df = pd.read_csv('labeled/con_post_label.csv', encoding= 'unicode_escape')\n",
    "con_df = con_df[~con_df['antivax'].isin([-1, 2, 3, 4])]\n",
    "input_ids, _, _ = encode_fn(con_df['Message'].to_list())\n",
    "con_preds = model(input_ids)\n",
    "formatted_preds = format_prediction(con_preds, label_mapping, 'antivax')\n",
    "con_preds = [p['antivax'] for p in formatted_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(y_true=con_df['antivax'], y_pred=con_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(y_true=con_df['antivax'], y_pred=con_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.precision_recall_fscore_support(y_true=con_df['antivax'], y_pred=con_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liberal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_df = pd.read_csv('labeled/lib_post_label.csv', encoding= 'unicode_escape')\n",
    "lib_df = lib_df[~lib_df['antivax'].isin([-1, 2])]\n",
    "input_ids, _, _ = encode_fn(lib_df['Message'].to_list())\n",
    "lib_preds = model(input_ids)\n",
    "formatted_preds = format_prediction(lib_preds, label_mapping, 'antivax')\n",
    "lib_preds = [p['antivax'] for p in formatted_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(y_true=lib_df['antivax'], y_pred=lib_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(y_true=lib_df['antivax'], y_pred=lib_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.precision_recall_fscore_support(y_true=lib_df['antivax'], y_pred=lib_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mom_df = pd.read_csv('labeled/mom_post_label.csv', encoding= 'unicode_escape')\n",
    "mom_df = mom_df[~mom_df['antivax'].isin([-1, 2])]\n",
    "input_ids, _, _ = encode_fn(mom_df['Message'].to_list())\n",
    "mom_preds = model(input_ids)\n",
    "formatted_preds = format_prediction(mom_preds, label_mapping, 'antivax')\n",
    "mom_preds = [p['antivax'] for p in formatted_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(y_true=mom_df['antivax'], y_pred=mom_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(y_true=mom_df['antivax'], y_pred=mom_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.precision_recall_fscore_support(y_true=mom_df['antivax'], y_pred=mom_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (covid-bert)",
   "language": "python",
   "name": "covid-bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
